<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Popular Science | Jennifer Renoux</title>
    <link>https://jrenoux.github.io/category/popular-science/</link>
      <atom:link href="https://jrenoux.github.io/category/popular-science/index.xml" rel="self" type="application/rss+xml" />
    <description>Popular Science</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 22 Aug 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jrenoux.github.io/media/logo_huc6f9f266f1d613c2d87b3693fb23c228_47872_300x300_fit_lanczos_3.png</url>
      <title>Popular Science</title>
      <link>https://jrenoux.github.io/category/popular-science/</link>
    </image>
    
    <item>
      <title>How do we communicate with robots?</title>
      <link>https://jrenoux.github.io/popular-science/how-communicate-robot/</link>
      <pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://jrenoux.github.io/popular-science/how-communicate-robot/</guid>
      <description>&lt;h2 id=&#34;i-have-been-writing-for-na-again-this-time-about-how-we-communicate-with-robots-the-article-in-swedish-is-available-herehttpswwwnase2025-08-22sa-kommunicerar-du-med-en-robot-in-this-page-is-the-english-version--enjoy-&#34;&gt;I have been writing for NA again! This time about how we communicate with robots. The article in Swedish is available &lt;a href=&#34;https://www.na.se/2025-08-22/sa-kommunicerar-du-med-en-robot/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. In this page is the English version ! Enjoy !&lt;/h2&gt;
&lt;p&gt;How do you know how to communicate? Communication is universal and innate. We were all born with the ability to communicate, and all humans do communicate. Animals communicate. Even plants communicate. We also see inter-species communication. We communicate with animals. But communication is also cultural and socially constructed. We do not communicate the same way in Sweden, in Japan, or in South Africa. We do not communicate the same way if we come from a rich or poor background. We do not communicate the same way at 10, 40, or 65 years old. And teenagers today certainly do not communicate the same way teenagers communicated 40 years ago.&lt;/p&gt;
&lt;p&gt;So what happens when our communication partner is not a human or an animal anymore, as it has been for millennia, but a robot? Robots, and more generally Artificial Intelligence, are strange entities. They are obviously not alive, but they are programmed to show some form of agency. So how do we communicate with an AI? As a researcher in Human-AI communication, this question is the core of my research interests.&lt;/p&gt;
&lt;p&gt;When we communicate with an AI, we will naturally employ some of the communication habits and constructs that we have learned during our previous interactions with humans and animals to guide us. We will try to make sense of this unknown partner through previous experiences. To do this, we will get influenced by how the AI looks and behave. If it looks childish, or cartoonish, we may talk to it as we would talk to a child. We will communicate to animal-resembling robots like we would with a pet. We will say &amp;ldquo;please&amp;rdquo; and &amp;ldquo;thank you&amp;rdquo; to ChatGPT. And while research has shown that drawing from these previous experiences might in fact help human-AI communication, it has also shown that we are not communicating exactly the same way with AI than we do with humans. For instance, a study in 2017 investigated how people interacted with an AI chatbot on a well-known social media compared to interactions with other humans. They figured out that humans employed different strategies when communicating with the AI than with the humans.&lt;/p&gt;
&lt;p&gt;How the AI system is programmed also has a strong impact on how a user will communicate with it. The persona displayed by an AI influences what characteristics we attribute to it. Does it present more male or female, is it friendly, assertive, arrogant? Is it more robotic or more human-like? And these decisions are entirely controlled by the people who develop these systems. An AI does not have the experience of a human being to draw from. It is a blank sheet on which a team of researchers and developers decide to write their ideas and values. And currently, the big push in the research community is to make communicative AI as human-like and &amp;ldquo;natural&amp;rdquo; as possible to facilitate communication. Researchers and engineers develop systems that communicate using language, that can show emotions and social cues in a natural way, all of this to make the interaction as natural as possible. But natural for whom?&lt;/p&gt;
&lt;p&gt;Have you ever heard of the W.E.I.R.D. acronym? It stands for &amp;ldquo;Western, Educated, Industrialized, Rich, Democratic.&amp;rdquo; It has been coined by the American anthropologist Joseph Heinrich and his collaborators, and  is used as way to summarize the background of most participants in psychological research, and by extension in Human-AI Interaction research. Most participants (and in fact, researchers) in these fields come from countries in the &amp;ldquo;West&amp;rdquo; (Western Europe or North America), with a high level of education, that are highly industrialized, globally rich, and generally democratic. His research has also shown that there are a lot of psychological differences between populations around the world and that W.E.I.R.D. populations are quite peculiar in their psychology. Therefore, the results drawn from research on these participants are not necessarily applicable to the population in all its diversity.&lt;/p&gt;
&lt;p&gt;This diversity issue also holds for other aspects of people’ life, and to the W.E.I.R.D. classification, I would also like to add &amp;ldquo;Able-bodied,&amp;rdquo; &amp;ldquo;Neurotypical,&amp;rdquo; and &amp;ldquo;English-speaking.&amp;rdquo; Research and development often fails to account for people with disabilities or neurodivergent, except when developing systems to &amp;ldquo;help&amp;rdquo; them. But on the contrary, systems most often end up exclusionary. Recent work has examined the guidelines that constructors were implementing in voice activated personal assistants (like Siri, Amazon’s Alexa, Google’s Echo&amp;hellip;). And they figured out that these guidelines are actively making usability worse for blind and visually impaired users. Blind and visually impaired users benefit greatly from voice assistants for every day tasks. But they also are able to process much faster speech and much more complex spoken sentences than sighted users. By trying to make the interaction more &amp;ldquo;natural&amp;rdquo; and using sighted people as default users, researchers and engineers exclude a very large part of the population. Another example is how ChatGPT fails to appropriately answer to people who speak dialects of English that are not American or British English. It usually ends up exaggerating the language’s features, failing to recognize the appropriate tone, thus answering in a far too formal way. Which in turn make the user feel uncomfortable and the tool feel useless.&lt;/p&gt;
&lt;p&gt;If we fail to take a large view on diversity, we will exclude a lot of people from the tool we develop. I want to advocate for a much wider view of human-AI communication. Spoken and written texts are not and should not be the ultimate medium for communication, especially when it comes to communicating with an AI system. Images, sounds, gestures, touch, are as many communication modalities that should be explored. So really, &lt;strong&gt;how&lt;/strong&gt; are we going to communicate with AI?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AI is a political topic - and it concerns you too</title>
      <link>https://jrenoux.github.io/popular-science/ai-is-political-issue/</link>
      <pubDate>Fri, 11 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://jrenoux.github.io/popular-science/ai-is-political-issue/</guid>
      <description>&lt;p&gt;Note : two versions of this article have been created and published, one in Swedish published in &lt;a href=&#34;https://www.na.se/2025-04-11/darfor-ar-ai-en-politisk-fraga-som-angar-dig/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nerikes Allehanda&lt;/a&gt;, and one in Frence published in &lt;a href=&#34;https://blogs.mediapart.fr/jennifer-renoux/blog/280425/lintelligence-artificielle-doit-devenir-une-consideration-citoyenne&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Le Club Mediapart&lt;/a&gt;. Note that this post is not an exact translation of these two articles but the original English version. The final articles have been shortened to fit the other newspapers format.&lt;/p&gt;
&lt;p&gt;&amp;ndash;&lt;/p&gt;
&lt;p&gt;We are being influenced by non-human entities that imitate humans, infiltrate human society, working on behalf of unknown organizations. Our opinions, our actions, our lives are being influenced by these entities. They are everywhere, in our home, at work, in our hospitals.
This is not the plot of a new Hollywood movie, nor some conspiratory rambling about a secret alien society. This is our world since the popularization of Artificial Intelligence (AI).&lt;/p&gt;
&lt;p&gt;Even though the entire world started to talk about Artificial Intelligence a bit more than 2 years ago with the release of ChatGPT, AI has been with us for much longer than that.
AI systems have been introduced in our lives with or without our knowledge and consent. Netflix or Spotify’s recommendation algorithms, who suggests you the next movie to watch or a nice playlist to listen to, are everyday examples. But more algorithms are impacting our lives than we think. Did you know that your mortgage request might have been granted by an AI system? Or that your resume might have been rejected by an AI? Or that the photo that stirred your emotion on the news outlet you read yesterday was AI generated?&lt;/p&gt;
&lt;p&gt;We are being exposed to AI, whether we want it or not. And AI is not neutral. People create or use an AI system to solve a problem. This could be too many mortgage requests or resumes  to go through for one person, the difficulty to find the right photo to reflect the tone of the article or even the intention to manipulate the reader’s emotion. But how the system actually ends up working depends on a social context : what the solution is supposed to look like, the data that is used to create this solution, the values that the system’s creator wants to uphold. And creators don’t always anticipate all the consequences of their choices.&lt;/p&gt;
&lt;p&gt;Algorithmic bias is the concept that an algorithm systematically creates unfair outcomes, privileging one group of people over another, usually in ways that are not the intended function of the algorithm. Common examples are &lt;a href=&#34;https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon’s recruiting tool that discriminated against women&lt;/a&gt;, &lt;a href=&#34;http://gendershades.org/overview.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;facial recognition algorithms that do not differentiate between different black persons&lt;/a&gt;, or &lt;a href=&#34;https://www.science.org/doi/10.1126/science.aax2342&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;medical systems that underestimate the medical needs of minorities&lt;/a&gt;. Algorithmic bias can have multiple causes. Often people will attribute this to data : if the data is biased, the system will be too. While this is true, it would be a mistake to think that data is the only culprit. For instance, in the United State, &lt;a href=&#34;https://www.science.org/doi/10.1126/science.aax2342&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hospitals have been using an AI system&lt;/a&gt; to help them decide whether an incoming patient should be sent to the emergency room or not based on their medical history. Understanding the medical history of a patient is a very complex problem, and the developers of the algorithm made the assumption that people with a severe medical history will spend more money on healthcare than others. They therefore used the amount of money previously spent on healthcare as a substitute for medical history. However, due to socio-economical reasons, black Americans spend on average less money on healthcare than white Americans with similar needs. Therefore, the algorithm systematically underestimated black American’s medical needs. Data itself is not the issue here.
It is important to note that algorithmic biases go much further than sexist or racist biases. For instance, researchers have been studying &lt;a href=&#34;https://arxiv.org/abs/2406.08818&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linguistic biases in ChatGPT&lt;/a&gt; and demonstrated that it is stereotyping dialects, which made users of these dialects feel uncomfortable and insulted.  Cultural biases or misrepresentations are other issues associated to AI systems.&lt;/p&gt;
&lt;p&gt;Why does it matter? After all, if we know these systems are biased, can’t we check the output ourselves and make our own mind? Unfortunately, it is not that simple. We all have a tendency to favor decisions made with automation compared to decision made without. This is called automation bias, and is well studied in psychology. This means that when AI systems present us with a solution, we are naturally inclined to adopt this solution. Yes, this applies even if you are aware that automation bias is a thing. Furthermore, researchers have also shown that using AI systems directly impacts our opinions. &lt;a href=&#34;https://arxiv.org/abs/2302.00560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In a study&lt;/a&gt;, a language model was programmed to promote a specific opinion towards social media. Participants who used this model wrote pieces that followed the promoted opinion, and even had their own stance towards social media influenced by the model. This is a frightening find. More and more text is being written with the help of ChatGPT and other AI assistants, and we have no control over the opinions programmed in them. Beyond text, AI systems are creating images, moderating content, and generally following rules and algorithms that we have no control over and belong to a handful of very rich companies. What we see, what we read, what we and our children take as role models. All of that is being fed to us by AIs, and we have no say in it.&lt;/p&gt;
&lt;p&gt;Or do we?&lt;/p&gt;
&lt;p&gt;AI must become a citizen concern. As we vote about economy, environment, social policies, we must also vote about AI, its development and its adoption. We don’t need to become AI experts any more that we needed to become geopolitical experts, or financial experts. However, we have a duty to educate ourselves, to understand who is talking about AI on the world’s stage and wonder about the consequences of what they are saying. We must reflect on what we want and how we want it to be. AI is now, more than ever, political. We cannot remain the passive observers of what is happening to us. We can and must have a say in it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
